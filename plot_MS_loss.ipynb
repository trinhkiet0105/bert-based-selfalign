{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\bert-based-selfalign\\self_align_bert\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RobertaModel:\n\tUnexpected key(s) in state_dict: \"embeddings.position_ids\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\bert-based-selfalign\\plot_MS_loss.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/bert-based-selfalign/plot_MS_loss.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 2. Load the saved weights\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/bert-based-selfalign/plot_MS_loss.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_best_batchsize144.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/bert-based-selfalign/plot_MS_loss.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/bert-based-selfalign/plot_MS_loss.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\bert-based-selfalign\\self_align_bert\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RobertaModel:\n\tUnexpected key(s) in state_dict: \"embeddings.position_ids\". "
     ]
    }
   ],
   "source": [
    "# 1. Create the model instance\n",
    "model = AutoModel.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model.to(device)\n",
    "# 2. Load the saved weights\n",
    "model_path = \"model_best_batchsize144.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "roberta_base_model= AutoModel.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "roberta_base_model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 50\n",
    "original_df = pd.read_csv(\"D:/MELD/MELD.Raw/MELD.Raw/dev_sent_emo.csv\")\n",
    "\n",
    "# Create an empty DataFrame to store the randomly selected rows\n",
    "selected_rows = []\n",
    "\n",
    "# Iterate over each unique emotion category\n",
    "unique_emotions =original_df['Emotion'].unique()\n",
    "for emotion in unique_emotions:\n",
    "    # Randomly select min_count rows for each emotion\n",
    "    sampled_rows = original_df[original_df['Emotion'] == emotion].sample(n=sample_num, random_state=random.randint(1,50),replace=True)\n",
    "    \n",
    "    # Append the sampled rows to the selected_rows list\n",
    "    selected_rows.append(sampled_rows)\n",
    "\n",
    "# Concatenate the selected rows into a new DataFrame\n",
    "selected_df = pd.concat(selected_rows)\n",
    "\n",
    "data = selected_df['Utterance'].to_list()\n",
    "labels = selected_df['Emotion'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.batch_encode_plus(\n",
    "    data,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=512  # or whatever max length you desire\n",
    ")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs['attention_mask']\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_output_embd(input_ids_list,attention_masks_list, model):\n",
    "    batch_size = 10\n",
    "\n",
    "    # Calculate the number of batches needed\n",
    "    num_batches = len(input_ids_list) // batch_size\n",
    "    if len(input_ids_list) % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    # Initialize an empty list to store the embeddings\n",
    "    all_embeddings_out2 = []\n",
    "\n",
    "    # Iterate over batches\n",
    "    for i in tqdm(range(num_batches)):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "\n",
    "        # Extract a batch of input data\n",
    "        batch_input_ids = input_ids_list[start_idx:end_idx]\n",
    "        batch_attention_masks = attention_masks_list[start_idx:end_idx]\n",
    "        batch_input_ids.to(device)\n",
    "        batch_attention_masks.to(device)\n",
    "        # Perform inference for the batch\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = model(batch_input_ids, attention_mask=batch_attention_masks).last_hidden_state[:,0,:]\n",
    "        all_embeddings_out2.append(batch_outputs)\n",
    "\n",
    "    # Concatenate the list of embeddings to get the final result\n",
    "    return torch.cat(all_embeddings_out2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_embeds = take_output_embd(input_ids,attention_mask,model=model)\n",
    "go_emo_embeds = take_output_embd(input_ids,attention_mask,model=roberta_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(embeddings, labels):\n",
    "    # Example data: Replace with your actual data\n",
    "    embeddings = embeddings.cpu().numpy()  \n",
    "\n",
    "    # Perform dimensionality reduction using t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Define a dictionary to map emotions to colors\n",
    "    emotion_to_color = {\n",
    "        'neutral': 'blue',\n",
    "        'surprise': 'green',\n",
    "        #'fear': 'red',\n",
    "        'sadness': 'purple',\n",
    "        'joy': 'yellow',\n",
    "        #'disgust': 'orange',\n",
    "        'anger': 'red'\n",
    "    }  # Define your own color mapping\n",
    "\n",
    "    # Map emotions to colors\n",
    "    colors = [emotion_to_color[emotion] for emotion in emotion_to_color.keys()]\n",
    "\n",
    "    # Create a scatter plot with points colored by labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for emotion, color in emotion_to_color.items():\n",
    "        mask = labels == emotion\n",
    "        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], c=color, label=emotion)\n",
    "\n",
    "    plt.title(\"t-SNE Visualization of Embeddings with Colored Emotions\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "\n",
    "    # Add a legend to the plot\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_pca(aligned_embeds, labels)\n",
    "plot_pca(go_emo_embeds, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_align_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
